{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-11T02:35:56.678352Z","iopub.execute_input":"2023-04-11T02:35:56.678846Z","iopub.status.idle":"2023-04-11T02:35:56.761411Z","shell.execute_reply.started":"2023-04-11T02:35:56.678797Z","shell.execute_reply":"2023-04-11T02:35:56.759948Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv\n/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as Data\nimport torch.nn.functional as F\n\ndtype = torch.FloatTensor\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:35:56.764022Z","iopub.execute_input":"2023-04-11T02:35:56.765114Z","iopub.status.idle":"2023-04-11T02:36:01.558791Z","shell.execute_reply.started":"2023-04-11T02:35:56.765073Z","shell.execute_reply":"2023-04-11T02:36:01.557664Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport time\nimport os\nimport torch.nn as nn\nimport torchtext.vocab as vocab\nimport torchtext.data as data\nfrom torchtext.data import *\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport spacy\nfrom keras.preprocessing import text\nfrom keras.utils import pad_sequences\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:01.560131Z","iopub.execute_input":"2023-04-11T02:36:01.567339Z","iopub.status.idle":"2023-04-11T02:36:39.906567Z","shell.execute_reply.started":"2023-04-11T02:36:01.567294Z","shell.execute_reply":"2023-04-11T02:36:39.905456Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"seed = 42\nBATCH_SIZE = 64\ntorch.manual_seed(seed)\nmax_len = 50\ntorch.backends.cudnn.deterministic = True\n#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nglove_path = \"../input/glove840b300dtxt/glove.840B.300d.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:39.911145Z","iopub.execute_input":"2023-04-11T02:36:39.911787Z","iopub.status.idle":"2023-04-11T02:36:39.920907Z","shell.execute_reply.started":"2023-04-11T02:36:39.911753Z","shell.execute_reply":"2023-04-11T02:36:39.919931Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def preprocess(data):\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    \n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text.lower()\n    \n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:39.922553Z","iopub.execute_input":"2023-04-11T02:36:39.923345Z","iopub.status.idle":"2023-04-11T02:36:39.935856Z","shell.execute_reply.started":"2023-04-11T02:36:39.923306Z","shell.execute_reply":"2023-04-11T02:36:39.934876Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep ='\\t')\ntest_data = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep ='\\t')\nsubmission = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:39.939506Z","iopub.execute_input":"2023-04-11T02:36:39.940546Z","iopub.status.idle":"2023-04-11T02:36:40.245304Z","shell.execute_reply.started":"2023-04-11T02:36:39.940506Z","shell.execute_reply":"2023-04-11T02:36:40.244251Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"x_train = preprocess(train_data['Phrase'])\nx_test = preprocess(test_data['Phrase'])\ny_train = train_data['Sentiment']","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:40.246711Z","iopub.execute_input":"2023-04-11T02:36:40.247077Z","iopub.status.idle":"2023-04-11T02:36:42.196026Z","shell.execute_reply.started":"2023-04-11T02:36:40.247039Z","shell.execute_reply":"2023-04-11T02:36:42.194604Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sentences = list(x_train )\nlabels = list(y_train )","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:42.197467Z","iopub.execute_input":"2023-04-11T02:36:42.197840Z","iopub.status.idle":"2023-04-11T02:36:42.229646Z","shell.execute_reply.started":"2023-04-11T02:36:42.197801Z","shell.execute_reply":"2023-04-11T02:36:42.228635Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"PAD = ' <PAD>'  # 未知字，padding符号用来填充长短不一的句子\npad_size =  50     # 每句话处理成的长度(短填长切)\n\nfor i in range(len(sentences)):\n    sen2list = sentences[i].split()\n    sentence_len = len(sen2list)\n    if sentence_len<pad_size:\n        sentences[i] += PAD*(pad_size-sentence_len)\n    else:\n        sentences[i] = \" \".join(sen2list[:pad_size])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:42.231022Z","iopub.execute_input":"2023-04-11T02:36:42.231651Z","iopub.status.idle":"2023-04-11T02:36:42.439538Z","shell.execute_reply.started":"2023-04-11T02:36:42.231612Z","shell.execute_reply":"2023-04-11T02:36:42.438298Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# TextCNN Parameter\nnum_classes = len(set(labels))  # num_classes=2\nbatch_size = 64\nword_list = \" \".join(sentences).split()\nvocab = list(set(word_list))\nword2idx = {w: i for i, w in enumerate(vocab)}\nvocab_size = len(vocab)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:42.442907Z","iopub.execute_input":"2023-04-11T02:36:42.443193Z","iopub.status.idle":"2023-04-11T02:36:43.378957Z","shell.execute_reply.started":"2023-04-11T02:36:42.443164Z","shell.execute_reply":"2023-04-11T02:36:43.377684Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def make_data(sentences, labels):\n    inputs = []\n    for sen in sentences:\n        inputs.append([word2idx[n] for n in sen.split()])\n\n    targets = []\n    for out in labels:\n        targets.append(out) # To using Torch Softmax Loss function\n    return inputs, targets\ninput_batch, target_batch = make_data(sentences, labels)\ninput_batch, target_batch = torch.LongTensor(input_batch), torch.LongTensor(target_batch)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:43.381072Z","iopub.execute_input":"2023-04-11T02:36:43.381811Z","iopub.status.idle":"2023-04-11T02:36:45.566256Z","shell.execute_reply.started":"2023-04-11T02:36:43.381772Z","shell.execute_reply":"2023-04-11T02:36:45.565146Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# 划分训练集，测试集\nx_train,x_test,y_train,y_test = train_test_split(input_batch,target_batch,test_size=0.2,random_state = 0)\n\ntrain_dataset = Data.TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\ntest_dataset = Data.TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\ndataset = Data.TensorDataset(input_batch, target_batch)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:45.567818Z","iopub.execute_input":"2023-04-11T02:36:45.568478Z","iopub.status.idle":"2023-04-11T02:36:45.748431Z","shell.execute_reply.started":"2023-04-11T02:36:45.568439Z","shell.execute_reply":"2023-04-11T02:36:45.747218Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \"\"\"\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader = Data.DataLoader(\n    dataset=train_dataset,      # 数据，封装进Data.TensorDataset()类的数据\n    batch_size=batch_size,      # 每块的大小\n    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n    #num_workers=2,              # 多进程（multiprocess）来读数据\n)\ntest_loader = Data.DataLoader(\n    dataset=test_dataset,      # 数据，封装进Data.TensorDataset()类的数据\n    batch_size=batch_size,      # 每块的大小\n    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n    #num_workers=2,              # 多进程（multiprocess）来读数据\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:45.749994Z","iopub.execute_input":"2023-04-11T02:36:45.750559Z","iopub.status.idle":"2023-04-11T02:36:45.757628Z","shell.execute_reply.started":"2023-04-11T02:36:45.750518Z","shell.execute_reply":"2023-04-11T02:36:45.756275Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class TextCNN(nn.Module):\n    def __init__(self):\n        super(TextCNN, self).__init__()\n        self.filter_sizes = (2, 3, 4)\n        self.embed = 300\n        self.num_filters = 256\n        self.dropout = 0.5\n        self.num_classes = num_classes\n        self.n_vocab = vocab_size\n        #通过padding_idx将<PAD>字符填充为0，因为他没意义哦，后面max-pooling自然而然会把他过滤掉哦\n        self.embedding = nn.Embedding(self.n_vocab, self.embed, padding_idx=word2idx['<PAD>'])\n        self.convs = nn.ModuleList(\n            [nn.Conv2d(1, self.num_filters, (k, self.embed)) for k in self.filter_sizes])\n        \n        self.dropout = nn.Dropout(self.dropout)\n        self.fc = nn.Linear(self.num_filters * len(self.filter_sizes), self.num_classes)\n        \n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n        \n    def forward(self, x):\n        out = self.embedding(x)\n        out = out.unsqueeze(1)\n        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n        out = self.dropout(out)\n        out = self.fc(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:45.759548Z","iopub.execute_input":"2023-04-11T02:36:45.760531Z","iopub.status.idle":"2023-04-11T02:36:45.772392Z","shell.execute_reply.started":"2023-04-11T02:36:45.760490Z","shell.execute_reply":"2023-04-11T02:36:45.771071Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = TextCNN().to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Training\nfor epoch in range(10):\n    acc = 0\n    for i, batch in enumerate(train_loader):\n        batch_x, batch_y = batch\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        pred = model(batch_x)\n        loss = criterion(pred, batch_y)\n        #acc += pred.eq(batch_y.view_as(pred)).sum().item()\n        if (i + 1) % 1000 == 0:\n            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss),'acc=','{:.3f}'.format(acc))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-11T02:36:45.773972Z","iopub.execute_input":"2023-04-11T02:36:45.774393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_acc_list = []\nmodel.eval()\ntest_loss = 0\ncorrect = 0\nwith torch.no_grad():\n     for i, batch in enumerate(test_loader):\n        batch_x, batch_y = batch\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        output = model(batch_x)\n#         loss = criterion(output, target)\n#         test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n\n        pred = output.max(1, keepdim=True)[1]                           # 找到概率最大的下标\n        correct += pred.eq(batch_y.view_as(pred)).sum().item()\n\n# test_loss /= len(test_loader.dataset)\n# test_loss_list.append(test_loss)\ntest_acc_list.append(100. * correct / len(test_loader.dataset))\nprint('Accuracy: {}/{} ({:.2f}%)\\n'.format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}