{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip',sep='\\t')\ntest=pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip',sep='\\t')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"siebert/sentiment-roberta-large-english\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import load_metric\n\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import AdamW\nfrom transformers import get_scheduler\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom tqdm.auto import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=5, ignore_mismatched_sizes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example, truncation=True)\n\ntokenized_datasets = train['Phrase'].map(tokenize_function)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment = train['Sentiment'].tolist()\n\nfor i in range(len(tokenized_datasets)):\n    tokenized_datasets[i]['label'] = sentiment[i]\nprint(tokenized_datasets[:2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    tokenized_datasets, shuffle=True, batch_size=64, collate_fn=data_collator\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr = 3e-5)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\nnum_epochs = 3\nnum_training_steps = num_epochs*len(train_dataloader)\nlr_scheduler = get_scheduler(\n    'linear',\n    optimizer = optimizer,\n    num_warmup_steps = 0,\n     num_training_steps =num_training_steps\n)\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\n\nfor epoch in range(num_epochs):\n     for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        print(loss)\n        \n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets_test = test[\"Phrase\"].map(tokenize_function)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataloader = DataLoader(\n    tokenized_datasets_test, batch_size=64, collate_fn=data_collator\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ntest_predictions = list()\n\nfor batch in test_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n\n    test_predictions.extend(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = [i.item()  for i in test_predictions]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id = test['PhraseId']\n\nprint(f\"test ids: {test_id[:4]}\")\nprint(f\"test preds: {test_predictions[:4]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(list(zip(test_id, test_predictions)),\n               columns =['PhraseId', 'Sentiment'])\nsubmission.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}